{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\n",
    "test  = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing data from train dataset\n",
    "total = train.isnull().sum().sort_values(ascending=False)\n",
    "percent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total,percent],axis=1,keys=['Total','Percent'])\n",
    "missing_data.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with missing data\n",
    "train = train.drop((missing_data[missing_data['Total']>1]).index,1)\n",
    "train['Electrical'] = train['Electrical'].fillna(train['Electrical'].mode()[0])\n",
    "train.isnull().sum().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After deleting the columns \n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing data from test dataset\n",
    "total = test.isnull().sum().sort_values(ascending=False)\n",
    "percent = (test.isnull().sum()/test.isnull().count()).sort_values(ascending=False)\n",
    "missing_data1 = pd.concat([total,percent],axis=1,keys=['Total','Percent'])\n",
    "missing_data1.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop((missing_data1[missing_data1['Total']>4]).index,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing data from test dataset\n",
    "total = test.isnull().sum().sort_values(ascending=False)\n",
    "percent = (test.isnull().sum()/test.isnull().count()).sort_values(ascending=False)\n",
    "missing_data1 = pd.concat([total,percent],axis=1,keys=['Total','Percent'])\n",
    "missing_data1.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_features = (missing_data1[missing_data1['Total']>0]).index\n",
    "null_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in null_features:\n",
    "    test[feature] = test[feature].fillna(test[feature].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descriptive statistics summary\n",
    "train['SalePrice'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram\n",
    "sns.distplot(train['SalePrice']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "corrmat = train.corr()\n",
    "f, ax = plt.subplots(figsize=(12,9))\n",
    "sns.heatmap(corrmat,vmax=.8,square = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most correlated features \n",
    "corrmat = train.corr()\n",
    "top_corr_features = corrmat.index[abs(corrmat['SalePrice'])>0.5]\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(train[top_corr_features].corr(),annot = True);\n",
    "top_corr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n",
    "sns.pairplot(train[cols], size = 2.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differentiate between numerical and categorical varibles\n",
    "categorical_features = train.select_dtypes(include = [\"object\"]).columns\n",
    "numerical_features = train.select_dtypes(exclude = [\"object\"]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking numerical dataset and categorical datasets separately \n",
    "train_num = train[numerical_features]\n",
    "train_cat = train[categorical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkin skewmess of all features\n",
    "skewness = train_num.apply(lambda x: skew(x))\n",
    "skewness.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness = skewness[abs(skewness)>0.5]\n",
    "skewness.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying log tranform\n",
    "train_num[skewness.index] = np.log1p(train_num[skewness.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using get_dummies here it is used for data manipulation. It converts categorical data into dummy or indicator variables\n",
    "train_cat = pd.get_dummies(train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating train_num (numerical variable) and train_cat (categorical variable)\n",
    "train1 = pd.concat([train_cat,train_num],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differentiate between numerical and categorical varibles\n",
    "categorical_features = test.select_dtypes(include = [\"object\"]).columns\n",
    "numerical_features = test.select_dtypes(exclude = [\"object\"]).columns\n",
    "\n",
    "test_num = train[numerical_features]\n",
    "test_cat = train[categorical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding skewness of all features\n",
    "skewness = test_num.apply(lambda x: skew(x))\n",
    "skewness.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are selecting features where skewness is greater than 0.5 to fix their skewness\n",
    "skewness = skewness[abs(skewness)>0.5]\n",
    "skewness.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying log tranform\n",
    "test_num[skewness.index] = np.log1p(test_num[skewness.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cat = pd.get_dummies(test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = pd.concat([test_cat,test_num],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set minimum and maximum threshold values to detect ouliers using standard deviation\n",
    "min_threshold = train1.SalePrice.mean() - 3*train1.SalePrice.std()\n",
    "max_threshold = train1.SalePrice.mean() + 3*train1.SalePrice.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_threshold,max_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the outlier's from dataset\n",
    "train1 = train1[(train1.SalePrice>max_threshold) & (train1.SalePrice)<min_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the required library for modeling\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train1.drop(['SalePrice'],axis=1)\n",
    "y = train1['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(X)\n",
    "test = sm.add_constant(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROOT MEAN SQUARED ERROR : \",math.sqrt(sum((y-predictions)**2)/len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a  DataFrame with the passengers ids and our prediction\n",
    "submission = pd.DataFrame({'Id':test_id,'SalePrice':y_pred_b})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert DataFrame to a csv file that can be uploaded\n",
    "#This is saved in the same directory as your notebook\n",
    "filename = './HousePredictions.csv'\n",
    "\n",
    "submission.to_csv(filename,index=False)\n",
    "\n",
    "#print('Saved file: ' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
